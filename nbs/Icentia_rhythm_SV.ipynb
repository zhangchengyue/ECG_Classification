{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafcebe0",
   "metadata": {},
   "source": [
    "# Convolutional autoencoder for image denoising\n",
    "[See Supplementary Table 3~5 for Model Details](https://static-content.springer.com/esm/art%3A10.1038%2Fs41746-020-00320-4/MediaObjects/41746_2020_320_MOESM1_ESM.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df590227",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60ab0e82",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute '_no_nep50_warning'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LearningRateScheduler\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\keras\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# DO NOT EDIT. Generated by api_gen.sh\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DTypePolicy\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FloatDTypePolicy\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Function\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\keras\\api\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\keras\\api\\activations\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\keras\\src\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\keras\\src\\activations\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m celu\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exponential\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\keras\\src\\activations\\activations.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\keras\\src\\backend\\__init__.py:38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Import backend functions.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Variable \u001b[38;5;28;01mas\u001b[39;00m BackendVariable\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribution_lib\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m image\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbuiltins\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcompiler\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtf2xla\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mxla\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dynamic_update_slice\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[32m     47\u001b[39m _tf2.enable()\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m function\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_api\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mv2\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__internal__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph_util\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\feature_column\\__init__.py:8\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"\"\"Public API for tf._api.v2.__internal__.feature_column namespace\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_column\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_column_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DenseColumn \u001b[38;5;66;03m# line: 1777\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_column\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_column_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureTransformationCache \u001b[38;5;66;03m# line: 1962\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_column\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_column_v2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SequenceDenseColumn \u001b[38;5;66;03m# line: 1941\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m readers\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01meager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_column\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column \u001b[38;5;28;01mas\u001b[39;00m fc_old\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_column\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m feature_column_v2_types \u001b[38;5;28;01mas\u001b[39;00m fc_types\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_column\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialization\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m sparse_tensor_lib\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframework\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensor_shape\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\python\\layers\\base.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlegacy_tf_layers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base\n\u001b[32m     18\u001b[39m InputSpec = base.InputSpec\n\u001b[32m     20\u001b[39m keras_style_scope = base.keras_style_scope\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\python\\keras\\__init__.py:25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minput_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequential\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\python\\keras\\models.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sequential\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training_v1\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AddMetric\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_layer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Layer\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training_arrays_v1\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training_distributed_v1\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m training_eager_v1\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py:37\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m issparse  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     39\u001b[39m   issparse = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\scipy\\sparse\\__init__.py:300\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Original code by Travis Oliphant.\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Modified and extended by Ed Schofield, Robert Cimrman,\u001b[39;00m\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Nathan Bell, and Jake Vanderplas.\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_warnings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_base\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_csc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\scipy\\sparse\\_base.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"Base class for sparse matrices\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_sputils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[32m      6\u001b[39m                        get_sum_dtype, isdense, isscalarlike,\n\u001b[32m      7\u001b[39m                        matrix, validateaxis, getdtype)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_matrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[32m     11\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33misspmatrix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33missparse\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msparray\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mSparseWarning\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSparseEfficiencyWarning\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prod\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m np_long, np_ulong\n\u001b[32m     13\u001b[39m __all__ = [\u001b[33m'\u001b[39m\u001b[33mupcast\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mgetdata\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misscalarlike\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misintlike\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     14\u001b[39m            \u001b[33m'\u001b[39m\u001b[33misshape\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33missequence\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33misdense\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mismatrix\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mget_sum_dtype\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m            \u001b[33m'\u001b[39m\u001b[33mbroadcast_shapes\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     17\u001b[39m supported_dtypes = [np.bool_, np.byte, np.ubyte, np.short, np.ushort, np.intc,\n\u001b[32m     18\u001b[39m                     np.uintc, np_long, np_ulong, np.longlong, np.ulonglong,\n\u001b[32m     19\u001b[39m                     np.float32, np.float64, np.longdouble,\n\u001b[32m     20\u001b[39m                     np.complex64, np.complex128, np.clongdouble]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\scipy\\_lib\\_util.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypeAlias, TypeVar\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_namespace, is_numpy, xp_size\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_docscrape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FunctionDoc, Parameter\n\u001b[32m     17\u001b[39m AxisError: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mException\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\scipy\\_lib\\_array_api.py:18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnpt\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_compat\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     is_array_api_obj,\n\u001b[32m     20\u001b[39m     size \u001b[38;5;28;01mas\u001b[39;00m xp_size,\n\u001b[32m     21\u001b[39m     numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat,\n\u001b[32m     22\u001b[39m     device \u001b[38;5;28;01mas\u001b[39;00m xp_device,\n\u001b[32m     23\u001b[39m     is_numpy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_numpy,\n\u001b[32m     24\u001b[39m     is_cupy_namespace \u001b[38;5;28;01mas\u001b[39;00m is_cupy,\n\u001b[32m     25\u001b[39m     is_torch_namespace \u001b[38;5;28;01mas\u001b[39;00m is_torch,\n\u001b[32m     26\u001b[39m     is_jax_namespace \u001b[38;5;28;01mas\u001b[39;00m is_jax,\n\u001b[32m     27\u001b[39m     is_array_api_strict_namespace \u001b[38;5;28;01mas\u001b[39;00m is_array_api_strict\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m __all__ = [\n\u001b[32m     31\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m_asarray\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33marray_namespace\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_almost_equal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33massert_array_almost_equal\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mget_xp_devices\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mxp_take_along_axis\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_unsupported_param_msg\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mxp_vector_norm\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     39\u001b[39m ]\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# To enable array API and strict array-like input validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\scipy\\_lib\\array_api_compat\\numpy\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m * \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from numpy import * doesn't overwrite these builtin names\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mround\u001b[39m \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\numpy\\testing\\__init__.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munittest\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TestCase\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _private\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_private\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_private\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_private\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extbuild\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\numpy\\testing\\_private\\utils.py:469\u001b[39m\n\u001b[32m    465\u001b[39m         pprint.pprint(desired, msg)\n\u001b[32m    466\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg.getvalue())\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m \u001b[38;5;129m@np\u001b[39m\u001b[43m.\u001b[49m\u001b[43m_no_nep50_warning\u001b[49m()\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34massert_almost_equal\u001b[39m(actual, desired, decimal=\u001b[32m7\u001b[39m, err_msg=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    471\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    472\u001b[39m \u001b[33;03m    Raises an AssertionError if two items are not equal up to desired\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[33;03m    precision.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    537\u001b[39m \n\u001b[32m    538\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    539\u001b[39m     __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Hide traceback for py.test\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\MScAC\\ECG_Classification\\.conda\\Lib\\site-packages\\numpy\\__init__.py:427\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchar\u001b[39;00m\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m char.chararray\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\u001b[34m__name__\u001b[39m, attr))\n",
      "\u001b[31mAttributeError\u001b[39m: module 'numpy' has no attribute '_no_nep50_warning'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import signal\n",
    "\n",
    "def normalize_signals(signals):\n",
    "    \"\"\"\n",
    "    Normalize signals to the [0, 1] range by scaling based on min and max.\n",
    "    \"\"\"\n",
    "    min_val = np.min(signals, axis=(1, 2), keepdims=True)\n",
    "    max_val = np.max(signals, axis=(1, 2), keepdims=True)\n",
    "    return (signals - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def bandpass_filter(signals, fs=200, lowcut=0.5, highcut=40.0, order=2):\n",
    "    \"\"\"\n",
    "    Apply a bandpass filter to the signals.\n",
    "    - fs: Sampling frequency\n",
    "    - lowcut: Low cutoff frequency\n",
    "    - highcut: High cutoff frequency\n",
    "    \"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    \n",
    "    # Create the bandpass filter\n",
    "    b, a = signal.butter(order, [low, high], btype='band')\n",
    "    \n",
    "    # Apply the filter to each signal in the dataset, maintaining original shape\n",
    "    filtered_signals = np.array([signal.filtfilt(b, a, sig.flatten()).reshape(sig.shape) for sig in signals])\n",
    "    \n",
    "    return filtered_signals\n",
    "\n",
    "\n",
    "def preprocess(signals):\n",
    "    \"\"\"Normalizes signals to [0, 1] range, then apply bandpass filter to remove baseline wandering and muscle artifacts.\"\"\"\n",
    "    signals = bandpass_filter(signals)\n",
    "    signals = normalize_signals(signals)\n",
    "    return signals\n",
    "\n",
    "\n",
    "def noise(array, noise_factor = 0.4):\n",
    "    \"\"\"Adds random noise to each image in the supplied array.\"\"\"\n",
    "    noisy_array = array + noise_factor * np.random.normal(\n",
    "        loc=0.0, scale=1.0, size=array.shape\n",
    "    )\n",
    "\n",
    "    return np.clip(noisy_array, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def display(array1, array2, n = 4):\n",
    "    \"\"\"Displays <n> random signals from each array.\"\"\"\n",
    "    indices = np.random.randint(len(array1), size=n)\n",
    "    signals1 = array1[indices, :]\n",
    "    signals2 = array2[indices, :]\n",
    "\n",
    "    plt.figure(figsize=(20, n))\n",
    "    for i, (signal1, signal2) in enumerate(zip(signals1, signals2)):\n",
    "        # Original signal\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.plot(signal1)\n",
    "        plt.title(\"Original\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # Noisy signal\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.plot(signal2)\n",
    "        plt.title(\"Noisy\")\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def check_nan(data):\n",
    "    \"\"\"Check whether there are samples with NaN in data.\"\"\"\n",
    "    with_nan = 0\n",
    "    nan_index = []\n",
    "    for i in range(len(data)):\n",
    "        if np.any(np.isnan(data[i])):  # Check whethere there are data with NaN\n",
    "            with_nan += 1\n",
    "            nan_index.append(i)\n",
    "    return with_nan, nan_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dcabae",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "Obtain the data from [https://www.synapse.org/Synapse:syn21985690](https://www.synapse.org/Synapse:syn21985690)\n",
    "You will need to sign in with an account to access it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "icentia_path = \"../data/icentia11k/train.npz\"\n",
    "icentia = np.load(icentia_path)\n",
    "icentia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff0b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the whole data into train, test, validate\n",
    "# Extract signals and labels\n",
    "signals = icentia[\"signal\"]\n",
    "rhythm_labels = icentia[\"rhythm\"]\n",
    "sv_labels = icentia[\"beat\"]\n",
    "\n",
    "# Split data into train (70%), temp (30%)\n",
    "train_data, temp_data, train_labels_rhythm, temp_labels_rhythm, train_labels_sv, temp_labels_sv = train_test_split(\n",
    "    signals, rhythm_labels, sv_labels, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Further split temp into test (15%) and validation (15%)\n",
    "test_data, validate_data, test_labels_rhythm, validate_labels_rhythm, test_labels_sv, validate_labels_sv = train_test_split(\n",
    "    temp_data, temp_labels_rhythm, temp_labels_sv, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# # Save train set\n",
    "# np.savez(\"../data/train.npz\", signal=train_data, rhythm=train_labels_rhythm, sv_label=train_labels_sv)\n",
    "\n",
    "# # Save test set\n",
    "# np.savez(\"../data/test.npz\", signal=test_data, rhythm=test_labels_rhythm, sv_label=test_labels_sv)\n",
    "\n",
    "# # Save validation set\n",
    "# np.savez(\"../data/validate.npz\", signal=validate_data, rhythm=validate_labels_rhythm, sv_label=validate_labels_sv)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(validate_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3927b16",
   "metadata": {},
   "source": [
    "## Remove NaN samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_nan(train_data))\n",
    "print(check_nan(test_data))\n",
    "print(check_nan(validate_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we check for NaNs across all dimensions except the first\n",
    "valid_train_mask = ~np.isnan(train_data).any(axis=tuple(range(1, train_data.ndim)))\n",
    "valid_test_mask = ~np.isnan(test_data).any(axis=tuple(range(1, test_data.ndim)))\n",
    "valid_validate_mask = ~np.isnan(validate_data).any(axis=tuple(range(1, validate_data.ndim)))\n",
    "\n",
    "# Apply the mask to remove rows with NaNs\n",
    "train_data = train_data[valid_train_mask]\n",
    "test_data = test_data[valid_test_mask]\n",
    "validate_data = validate_data[valid_validate_mask]\n",
    "\n",
    "train_labels_rhythm = train_labels_rhythm[valid_train_mask]\n",
    "test_labels_rhythm = test_labels_rhythm[valid_test_mask]\n",
    "validate_labels_rhythm = validate_labels_rhythm[valid_validate_mask]\n",
    "\n",
    "train_labels_sv = train_labels_sv[valid_train_mask]\n",
    "test_labels_sv = test_labels_sv[valid_test_mask]\n",
    "validate_labels_sv = validate_labels_sv[valid_validate_mask]\n",
    "\n",
    "print(\"Train data subset shape after removing NaN:\", train_data.shape)\n",
    "print(\"Test data subset shape after removing NaN:\", test_data.shape)\n",
    "print(\"Validate data subset shape after removing NaN:\", validate_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdcf1aa",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d34fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(train_data)\n",
    "test_data = preprocess(test_data)\n",
    "validate_data = preprocess(validate_data)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(validate_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cfb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the data with added noise\n",
    "noise_levels = [0.001, 0.25, 0.5, 0.75, 1, 2, 5]\n",
    "noisy_train_data = []\n",
    "noisy_test_data = []\n",
    "noisy_validate_data = []\n",
    "for noise_factor in noise_levels:\n",
    "    noisy_train_data.append(noise(train_data, noise_factor = noise_factor))\n",
    "    noisy_test_data.append(noise(test_data, noise_factor = noise_factor))\n",
    "    noisy_validate_data.append(noise(validate_data, noise_factor = noise_factor))\n",
    "\n",
    "# Display the train data with added noise\n",
    "for i in range(len(noise_levels)):\n",
    "    print(f\"Noise Factor: {noise_levels[i]}\")\n",
    "    display(train_data, noisy_train_data[i])\n",
    "\n",
    "# Comcatenate all noisy images to become a whole dataset\n",
    "simulated_noisy_train_data = np.concatenate(noisy_train_data, axis=0)\n",
    "simulated_noisy_test_data = np.concatenate(noisy_test_data, axis=0)\n",
    "simulated_noisy_validate_data = np.concatenate(noisy_validate_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure there's no NaN samples\n",
    "print(check_nan(simulated_noisy_train_data))\n",
    "print(check_nan(simulated_noisy_test_data))\n",
    "print(check_nan(simulated_noisy_validate_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cd09b",
   "metadata": {},
   "source": [
    "## Build the autoencoder\n",
    "\n",
    "We are going to use the Functional API to build our convolutional autoencoder.\n",
    "\n",
    "[See Supplementary Table 3 for CDAE Model Details](https://static-content.springer.com/esm/art%3A10.1038%2Fs41746-020-00320-4/MediaObjects/41746_2020_320_MOESM1_ESM.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c187796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.initializers import he_normal\n",
    "\n",
    "# Input Layer\n",
    "input_layer = layers.Input(shape=(800, 1)) \n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv1D(filters=64, kernel_size=10, activation='relu', padding='same', kernel_initializer=he_normal())(input_layer)\n",
    "x = layers.MaxPooling1D(pool_size=3)(x)  \n",
    "x = layers.Conv1D(filters=45, kernel_size=8, activation='relu', padding='same', kernel_initializer=he_normal())(x)\n",
    "x = layers.MaxPooling1D(pool_size=3)(x)  \n",
    "x = layers.Conv1D(filters=50, kernel_size=5, activation='relu', padding='same', kernel_initializer=he_normal())(x)\n",
    "cdae_encoder = layers.MaxPooling1D(pool_size=2)(x) \n",
    "\n",
    "# Decoder\n",
    "x = layers.Conv1D(filters=50, kernel_size=5, activation='relu', padding='same', kernel_initializer=he_normal())(cdae_encoder)\n",
    "x = layers.UpSampling1D(size=2)(x) \n",
    "x = layers.Conv1D(filters=45, kernel_size=8, activation='relu', padding='same', kernel_initializer=he_normal())(x)\n",
    "x = layers.UpSampling1D(size=3)(x)  \n",
    "x = layers.Conv1D(filters=64, kernel_size=10, activation='relu', padding='same', kernel_initializer=he_normal())(x)\n",
    "x = layers.UpSampling1D(size=3)(x)  \n",
    "\n",
    "# Flatten\n",
    "x = layers.Flatten()(x)  # (None, 792, 64) -> (None, 792 * 64) = (None, 50688)\n",
    "\n",
    "# Dense\n",
    "x = layers.Dense(units=800, activation='sigmoid', kernel_initializer=he_normal())(x)  # (None, 50688) -> (None, 800)\n",
    "\n",
    "# Reshape to match input shape\n",
    "x = layers.Reshape((800, 1))(x)  # (None, 800) -> (None, 800, 1)\n",
    "\n",
    "# Build CDAE model\n",
    "cdae = Model(inputs=input_layer, outputs=x)\n",
    "cdae.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print structure\n",
    "cdae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730869d",
   "metadata": {},
   "source": [
    "## Train CDAE\n",
    "\n",
    "Now we can train our autoencoder using the noisy\n",
    "data as our input and the clean data as our target. We want our autoencoder to\n",
    "learn how to denoise the images. Notice we are setting up the validation data using the same\n",
    "format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"simulated_noisy_train_data shape:\", simulated_noisy_train_data.shape)\n",
    "print(\"train_data shape before tiling:\", train_data.shape)\n",
    "print(\"y shape after tiling:\", np.tile(train_data, (len(noise_levels), 1, 1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2514100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial learning rate\n",
    "initial_lr = 0.01\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch % 25 == 0 and epoch > 0:\n",
    "        return max(lr - 0.001, 0.0001)  # Make sure the learning rate does not become negative\n",
    "    return lr\n",
    "\n",
    "lr_reduction = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "# Train Autoencoder\n",
    "history = cdae.fit(\n",
    "    # The input to the CDAE was the simulated signal dataset\n",
    "    # with a Gaussian noise factor of 0.001, 0.5, 0.25, 0.75,\n",
    "    # 1, 2, and 5 added to corrupt the simulated signals.\n",
    "    x=simulated_noisy_train_data,\n",
    "\n",
    "    # The uncorrupted simulated signals are then used as the\n",
    "    # target for reconstruction.\n",
    "    y=np.tile(train_data, (len(noise_levels), 1, 1)),\n",
    "    epochs=3,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(simulated_noisy_validate_data, np.tile(validate_data, (len(noise_levels), 1, 1))),\n",
    "    callbacks=[lr_reduction]\n",
    ")\n",
    "\n",
    "print(history.history) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c0bbef",
   "metadata": {},
   "source": [
    "## Use CDAE for prediction\n",
    "\n",
    "Let's now predict on the noisy data and display the results of our autoencoder.\n",
    "\n",
    "Notice how the autoencoder does an amazing job at removing the noise from the\n",
    "input images.\n",
    "\n",
    "Since now I'm only using a very tiny subset of the dataset, it makes sense that the model is performing very badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cdae.predict(test_data)\n",
    "display(test_data, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cdc84b",
   "metadata": {},
   "source": [
    "## Save Pre-trained Weights\n",
    "To save time, the pre-trained weights are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = cdae.get_weights()\n",
    "# np.savez(\"../data/icentia11k/pretrained_weights/pretrained_weights_200epochs.npz\", *weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58214faa",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "[See here for tutorial](https://keras.io/guides/transfer_learning/)\n",
    "\n",
    "The typical transfer-learning workflow\n",
    "\n",
    "This leads us to how a typical transfer learning workflow can be implemented in Keras:\n",
    "\n",
    "1. Instantiate a base model and load pre-trained weights (from pre-trained CDAE as illustrated above) into it.\n",
    "2. Freeze all layers in the base model by setting <code>trainable = False</code>.\n",
    "3. Create a new model (DeepBeat Architecture) on top of the output of one (or several) layers from the base model.\n",
    "4. Train your new model on your new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c02401",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain pre-trained CDAE encoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8920773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_weights = np.load(\"../data/icentia11k/pretrained_weights/pretrained_weights_200epochs.npz\")\n",
    "# cdae_encoder_weights = [pretrained_weights[f\"arr_{i}\"] for i in range(6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f4009f",
   "metadata": {},
   "source": [
    "# DeepBeat Architecture (Multi-Task Learning)\n",
    "TODO: Modify the code based on the workflow.\n",
    "\n",
    "[See Supplementary Table 5 for Model Details](https://static-content.springer.com/esm/art%3A10.1038%2Fs41746-020-00320-4/MediaObjects/41746_2020_320_MOESM1_ESM.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0a1e4",
   "metadata": {},
   "source": [
    "## Shared Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "input_layer = layers.Input(shape=(800, 1)) \n",
    "\n",
    "# 1. Encoder (Base Model).\n",
    "# Use pre-trained CDAE encoder weights here\n",
    "x = layers.Conv1D(filters=64, kernel_size = 10, activation='relu', padding='same')(input_layer)\n",
    "x = layers.MaxPooling1D(pool_size=3)(x)  \n",
    "x = layers.Conv1D(filters=45, kernel_size = 8, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling1D(pool_size=3)(x)  \n",
    "x = layers.Conv1D(filters=50, kernel_size=5, activation='relu', padding='same')(x)\n",
    "base_model = layers.MaxPooling1D(pool_size=2)(x) \n",
    "\n",
    "# 2. Define model\n",
    "encoder_model = Model(inputs=input_layer, outputs=base_model, name=\"encoder_model\")\n",
    "\n",
    "# 3. Load pre-trained encoder weights\n",
    "weights = cdae.get_weights()\n",
    "encoder_model.set_weights(weights[:6])\n",
    "\n",
    "# 4. Freeze encoder layers\n",
    "encoder_model.trainable = False  \n",
    "\n",
    "# 5. Build base model\n",
    "base_model = encoder_model(input_layer)\n",
    "\n",
    "# 3. Create new model on top of base model\n",
    "# DeepBeat Architecture:\n",
    "# Shared Layers\n",
    "shared_input = layers.BatchNormalization()(base_model)\n",
    "\n",
    "x = layers.Conv1D(filters=64, kernel_size=6, strides = 3, padding='same')(shared_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Conv1D(filters=35, kernel_size=5, strides = 3, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Conv1D(filters=64, kernel_size=5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "shared_layers = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Build shared_layers model\n",
    "shared = Model(inputs=shared_input, outputs=shared_layers, name = \"DeepBeat_shared_layers\")\n",
    "shared.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print structure\n",
    "shared.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b439fb",
   "metadata": {},
   "source": [
    "## Rhythm Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rythm = layers.Conv1D(filters=35, kernel_size=2, strides = 3, padding='same')(shared_layers)\n",
    "rythm = layers.BatchNormalization()(rythm)\n",
    "rythm = layers.Dropout(0.5)(rythm)\n",
    "\n",
    "rythm = layers.Conv1D(filters=25, kernel_size=2, strides = 3, padding='same')(rythm)\n",
    "rythm = layers.BatchNormalization()(rythm)\n",
    "rythm = layers.Dropout(0.5)(rythm)\n",
    "\n",
    "rythm = layers.Conv1D(filters=35, kernel_size=2, padding='same')(rythm)\n",
    "rythm = layers.BatchNormalization()(rythm)\n",
    "rythm = layers.Dropout(0.5)(rythm)\n",
    "\n",
    "rythm = layers.Flatten()(rythm)\n",
    "rythm = layers.Dense(175, activation='relu')(rythm)\n",
    "rythm = layers.Dense(2, activation='softmax')(rythm)\n",
    "\n",
    "# Build rhythm_branch model\n",
    "rhythm_branch = Model(inputs=input_layer, outputs=rythm, name = \"Rhythm_Branch\")\n",
    "rhythm_branch.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Print structure\n",
    "rhythm_branch.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bba23",
   "metadata": {},
   "source": [
    "## Supraventricular (S) / Ventricular (V) Ectopic Beat Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fad4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Conv Layers\n",
    "sv = layers.Conv1D(filters=64, kernel_size=5, strides=2, padding='same', activation='relu')(shared_layers)\n",
    "sv = layers.BatchNormalization()(sv)\n",
    "sv = layers.Dropout(0.5)(sv)\n",
    "\n",
    "sv = layers.Conv1D(filters=48, kernel_size=3, strides=2, padding='same', activation='relu')(sv)\n",
    "sv = layers.BatchNormalization()(sv)\n",
    "sv = layers.Dropout(0.5)(sv)\n",
    "\n",
    "sv = layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(sv)\n",
    "sv = layers.BatchNormalization()(sv)\n",
    "sv = layers.Dropout(0.5)(sv)\n",
    "\n",
    "# Flatten and Dense Layers\n",
    "sv = layers.Flatten()(sv)\n",
    "sv = layers.Dense(128, activation='relu')(sv)\n",
    "sv = layers.Dense(2, activation='sigmoid')(sv)  # Sigmoid for binary classification\n",
    "\n",
    "# Build Model\n",
    "sv_branch = Model(inputs=input_layer, outputs=sv, name=\"S_V_Classification\")\n",
    "\n",
    "# Compile Model\n",
    "sv_branch.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "sv_branch.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9e802",
   "metadata": {},
   "source": [
    "## The complete multi-task learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f7c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask = Model(inputs=input_layer, outputs=[rythm, sv], name = \"Multi-task_learning_model\")\n",
    "multitask.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print structure\n",
    "multitask.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f7ad8",
   "metadata": {},
   "source": [
    "## Train each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9c352",
   "metadata": {},
   "source": [
    "### Rhythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(np.argmax(train_labels_rhythm, axis=1))\n",
    "total = neg + pos\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "rhythm_class_0 = (1 / neg) * (total / 2.0)\n",
    "rhythm_class_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "rhythm_class_weight = {0: rhythm_class_0, 1: rhythm_class_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(rhythm_class_0))\n",
    "print('Weight for class 1: {:.2f}'.format(rhythm_class_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1248cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial learning rate\n",
    "initial_lr = 0.01\n",
    "\n",
    "lr_reduction = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "# Train Autoencoder\n",
    "rhythm_history = rhythm_branch.fit(\n",
    "    x=train_data,\n",
    "    y= train_labels_rhythm,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(validate_data, validate_labels_rhythm),\n",
    "    callbacks=[lr_reduction],\n",
    "    class_weight = rhythm_class_weight\n",
    ")\n",
    "\n",
    "print(rhythm_history.history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a848a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhythm_predictions = rhythm_branch.predict(test_data)\n",
    "display(test_data, rhythm_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66639df",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_rhythm_labels = np.argmax(rhythm_predictions, axis=1)\n",
    "\n",
    "predicted_rhythm_labels_one_hot = np.eye(2)[predicted_rhythm_labels]\n",
    "\n",
    "print(\"Predicted Rhythm labels:\")\n",
    "print(predicted_rhythm_labels_one_hot[:5])\n",
    "\n",
    "print(\"True Rhythm labels:\")\n",
    "print(test_labels_rhythm[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e70f6",
   "metadata": {},
   "source": [
    "### SV Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d22008",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = np.bincount(np.argmax(train_labels_sv, axis=1))\n",
    "total = neg + pos\n",
    "\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "sv_class_0 = (1 / neg) * (total / 2.0)\n",
    "sv_class_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "sv_class_weight = {0: sv_class_0, 1: sv_class_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(sv_class_0))\n",
    "print('Weight for class 1: {:.2f}'.format(sv_class_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585d3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial learning rate\n",
    "initial_lr = 0.01\n",
    "\n",
    "lr_reduction = LearningRateScheduler(lr_schedule, verbose=1)\n",
    "\n",
    "# Train Autoencoder\n",
    "sv_history = sv_branch.fit(\n",
    "    x=train_data,\n",
    "    y = train_labels_sv,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    validation_data=(validate_data, validate_labels_sv),\n",
    "    callbacks=[lr_reduction],\n",
    "    class_weight = sv_class_weight\n",
    ")\n",
    "\n",
    "print(sv_history.history) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_predictions = sv_branch.predict(test_data)\n",
    "display(test_data, sv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sv_labels = np.argmax(sv_predictions, axis=1)\n",
    "\n",
    "predicted_sv_labels_one_hot = np.eye(3)[predicted_sv_labels]\n",
    "\n",
    "print(\"Predicted SV labels:\")\n",
    "print(predicted_sv_labels_one_hot[:5])\n",
    "\n",
    "print(\"True SV labels:\")\n",
    "print(test_labels_sv[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6018b",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab855716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, f1_score, recall_score, \n",
    "    precision_score, accuracy_score\n",
    ")\n",
    "\n",
    "# Convert one-hot encoding to class labels\n",
    "y_true_rhythm_cls = np.argmax(test_labels_rhythm, axis=1)\n",
    "y_pred_rhythm_cls = predicted_rhythm_labels\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true_rhythm_cls, y_pred_rhythm_cls)\n",
    "\n",
    "# Extract TP, FP, FN, TN for binary classification\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Compute performance metrics\n",
    "sensitivity = TP / (TP + FN)  # Recall\n",
    "specificity = TN / (TN + FP)\n",
    "fpr = FP / (FP + TN)  # False Positive Rate\n",
    "fnr = FN / (FN + TP)  # False Negative Rate\n",
    "f1 = f1_score(y_true_rhythm_cls, y_pred_rhythm_cls, average=\"macro\")  # Macro F1-score\n",
    "accuracy = accuracy_score(y_true_rhythm_cls, y_pred_rhythm_cls)  # Accuracy\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "\n",
    "# --- Weighted Macro-Averaged (Handling Class Imbalance) ---\n",
    "f1_weighted = f1_score(y_true_rhythm_cls, y_pred_rhythm_cls, average=\"weighted\")\n",
    "recall_weighted = recall_score(y_true_rhythm_cls, y_pred_rhythm_cls, average=\"weighted\")\n",
    "precision_weighted = precision_score(y_true_rhythm_cls, y_pred_rhythm_cls, average=\"weighted\")\n",
    "\n",
    "print(\"\\n--- Weighted Macro-Averaged Metrics ---\")\n",
    "print(f\"Weighted F1 Score: {f1_weighted:.4f}\")\n",
    "print(f\"Weighted Recall (Sensitivity): {recall_weighted:.4f}\")\n",
    "print(f\"Weighted Precision: {precision_weighted:.4f}\")\n",
    "\n",
    "# Generate full classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_rhythm_cls, y_pred_rhythm_cls))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert one-hot encoding to class labels\n",
    "y_true_sv_cls = np.argmax(test_labels_sv, axis=1)\n",
    "y_pred_sv_cls = predicted_sv_labels\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true_sv_cls, y_pred_sv_cls)\n",
    "\n",
    "# Extract TP, FP, FN, TN for binary classification\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "# Compute performance metrics\n",
    "sensitivity = TP / (TP + FN)  # Recall\n",
    "specificity = TN / (TN + FP)\n",
    "fpr = FP / (FP + TN)  # False Positive Rate\n",
    "fnr = FN / (FN + TP)  # False Negative Rate\n",
    "f1 = f1_score(y_true_sv_cls, y_pred_sv_cls, average=\"macro\")  # Macro F1-score\n",
    "accuracy = accuracy_score(y_true_sv_cls, y_pred_sv_cls)  # Accuracy\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1:.4f}\")\n",
    "\n",
    "# --- Weighted Macro-Averaged (Handling Class Imbalance) ---\n",
    "f1_weighted = f1_score(y_true_sv_cls, y_pred_sv_cls, average=\"weighted\")\n",
    "recall_weighted = recall_score(y_true_sv_cls, y_pred_sv_cls, average=\"weighted\")\n",
    "precision_weighted = precision_score(y_true_sv_cls, y_pred_sv_cls, average=\"weighted\")\n",
    "\n",
    "print(\"\\n--- Weighted Macro-Averaged Metrics ---\")\n",
    "print(f\"Weighted F1 Score: {f1_weighted:.4f}\")\n",
    "print(f\"Weighted Recall (Sensitivity): {recall_weighted:.4f}\")\n",
    "print(f\"Weighted Precision: {precision_weighted:.4f}\")\n",
    "\n",
    "# Generate full classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_sv_cls, y_pred_sv_cls))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8325eea3",
   "metadata": {},
   "source": [
    "## Plot ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "\n",
    "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "  plt.xlabel('False positives [%]')\n",
    "  plt.ylabel('True positives [%]')\n",
    "  plt.grid(True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_aspect('equal')\n",
    "\n",
    "labels_rhythm = np.argmax(test_labels_rhythm, axis=1)         # shape: (N,)\n",
    "\n",
    "labels_sv = np.argmax(test_labels_sv, axis=1)\n",
    "\n",
    "plot_roc(\"Rhythm Test Baseline\", labels_rhythm, predicted_rhythm_labels, linestyle='--')\n",
    "plot_roc(\"Beat Test Baseline\", labels_sv, predicted_sv_labels, linestyle='--')\n",
    "plt.legend(loc='lower right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
